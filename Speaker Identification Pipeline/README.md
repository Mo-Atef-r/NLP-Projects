# Speaker Identification Pipeline (Voice Biometrics)

## üöÄ Project Overview

This project implements a complete, end-to-end **speaker identification pipeline** utilizing a pre-trained **ECAPA-TDNN deep learning model** from the SpeechBrain toolkit. It focuses on identifying known speakers from audio inputs and robustly rejecting unknown or unauthorized individuals, forming the core of a voice biometric system.

## üõ†Ô∏è Methodology & Pipeline Stages

The pipeline encompasses several critical stages:

* **Data Preparation:**
    * Meticulously filters and organizes audio files from the **RAVDESS emotional speech dataset** (`Actor_01` to `Actor_24`).
    * Splits data into distinct enrollment (training) and test sets for **enrolled actors** (Actors 01-06) based on specific criteria (e.g., statement, emotion, repetition).
    * Prepares a large set of **"unknown" (imposter) actor** data (Actors 07-24) for robust imposter rejection testing, covering all statement 1 files with diverse emotions and intensities.

* **Speaker Enrollment:**
    * Generates unique **voiceprint embeddings** for each enrolled speaker.
    * This is achieved by extracting and averaging multiple embeddings from their designated enrollment audio files, creating a robust representation.

* **1:N Speaker Identification:**
    * Develops a core function to identify a speaker from any new audio input.
    * Compares the incoming audio's embedding against **all enrolled voiceprints** using cosine similarity.
    * Applies a dynamically calibrated similarity threshold to make identification decisions (accept/reject).

* **Imposter Rejection Testing:**
    * Performs extensive evaluation of the model's ability to reject **unknown (imposter) speakers**.
    * Tests against a large, dedicated set of unenrolled actors (Actors 07-24) to quantify the system's robustness against unauthorized access attempts.

## üìà Key Findings & Achievements

* **High Imposter Rejection Accuracy:** Achieved a remarkable **95.7% accuracy** in correctly rejecting unseen and unenrolled actors (imposters) across a diverse set of audio files from Actors 07-24.
* **Identified Model Nuances:** Through detailed analysis, observed specific instances of potential confusion, notably between **Actor 01 and Actor 23**. This highlights areas for future model refinement or the need for more diverse enrollment data to enhance intra-class separation.
* **Threshold Optimization Insight:** Demonstrated the critical importance and practical utility of adjusting the detection threshold. This allows for strategic prioritization, such as minimizing False Acceptances for security-sensitive applications, even at the potential cost of slightly higher False Rejection Rates.

## üìö Dataset Citation

This project utilizes the **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)** dataset.

Original Source:
* Livingstone, S. R., & Russo, F. A. (2018). The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English for research on emotion perception. PLoS ONE, 13(5), e0196391.

Dataset hosted on Kaggle:
* [RAVDESS Emotional Speech Audio Dataset](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio/data)

---
